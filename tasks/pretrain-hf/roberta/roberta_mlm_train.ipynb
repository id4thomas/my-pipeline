{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta-mlm-train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --quiet\n",
        "!pip install torch\n",
        "!git clone https://github.com/cardiffnlp/tweeteval /tmp/tweeteval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJqbBtN4TwuO",
        "outputId": "ac58673f-27c6-45ce-f2b1-23e99dd00b44"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "fatal: destination path '/tmp/tweeteval' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vQWb36q9TUrf"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForMaskedLM.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"roberta-base\",\n",
        "    tokenizer=\"roberta-base\"\n",
        ")\n",
        "fill_mask(\"Send these <mask> back!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inVzKMPeXOhZ",
        "outputId": "e3415989-526e-4b80-9114-d86ca3a71920"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.16661465167999268,\n",
              "  'sequence': 'Send these pictures back!',\n",
              "  'token': 3493,\n",
              "  'token_str': ' pictures'},\n",
              " {'score': 0.10792841017246246,\n",
              "  'sequence': 'Send these photos back!',\n",
              "  'token': 2356,\n",
              "  'token_str': ' photos'},\n",
              " {'score': 0.07670916616916656,\n",
              "  'sequence': 'Send these emails back!',\n",
              "  'token': 5575,\n",
              "  'token_str': ' emails'},\n",
              " {'score': 0.04860782250761986,\n",
              "  'sequence': 'Send these images back!',\n",
              "  'token': 3156,\n",
              "  'token_str': ' images'},\n",
              " {'score': 0.048417482525110245,\n",
              "  'sequence': 'Send these letters back!',\n",
              "  'token': 5430,\n",
              "  'token_str': ' letters'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/tmp/tweeteval/datasets/hate/train_text.txt\",\n",
        "    block_size=512,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42Sfg5VATjPV",
        "outputId": "4b6501ee-4428-4e0e-d49c-12513f1b4126"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxE2HHL7WMa-",
        "outputId": "d0b0ee37-76ba-4b97-d255-0e7b6d1210d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'input_ids': tensor([    0,  1039, 12105,  2579,    92, 25152,     4,  3945,    47,    45,\n",
            "         2273,    30, 10841,   459, 34865,   111,  5827, 38561,  8817,  3977,\n",
            "         1657,  4950,  1295,    15,    47,  1174,  1437,     2])}, {'input_ids': tensor([    0,   250,   693,    54,    47, 42647,  1533,   498,   584, 25610,\n",
            "        38594,   650,    16,    10, 19389,    47,   216,  1717,   478,    14,\n",
            "         1514, 17841, 12736,  1437,     2])}, {'input_ids': tensor([    0,  1039, 12105,   787, 12105,   588,  1067,   109,    47,    33,\n",
            "         2473,    50,    58,    51, 34348,  4462,    66,    30,    10,  5345,\n",
            "          506, 11797,   242,   116,  1437,     2])}, {'input_ids': tensor([    0, 16625,  6096,   356,   179,    23,   162,   101,    10,   333,\n",
            "          324,    11,    42, 32594,   328,  1437,     2])}, {'input_ids': tensor([    0, 30811,  3121,  3569,   693,   101,   787, 12105,  1437,     2])}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head /tmp/tweeteval/datasets/hate/train_text.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK_6wUAKWsa0",
        "outputId": "86f5a29a-5c37-4621-e62d-4e5cfeefe36e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on youâ€¦ \n",
            "A woman who you fucked multiple times saying yo dick small is a compliment you know u hit that spot ðŸ˜Ž \n",
            "@user @user real talk do you have eyes or were they gouged out by a rapefugee? \n",
            "your girlfriend lookin at me like a groupie in this bitch! \n",
            "Hysterical woman like @user \n",
            "Me flirting- So tell me about your father... \n",
            "The Philippine Catholic bishops' work for migrant workers should focus on families who are \"paying the great... \n",
            "I AM NOT GOING AFTER YOUR EX BF YOU LIEING SACK OF SHIT ! I'm done with you dude that's why I dumped your ass cause your a lieing ðŸ˜‚ðŸ˜¡ bitch \n",
            "When cuffin season is finally over \n",
            "Send home migrants not in need of protection, Peter Dutton tells UN, HEY DUTTON HOW ABOUT THE ONES THAT HAVE STAYED AND NOT LEFT THE COUNTRY WHEN THEY SHOULD OVERSTAYERS ? WHY DONT YOU GO AND ROUND ALL THEM UP ?  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "metadata": {
        "id": "rDnGcQPkTl5R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta-retrained\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")"
      ],
      "metadata": {
        "id": "ZVRxj7CRTnbK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./roberta-retrained\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "rQtHsN4UTpD1",
        "outputId": "e5d466e0-0c42-4aa8-9c1b-ea9d85579ec5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 8993\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1410\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1410' max='1410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1410/1410 14:33, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.415000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.121000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./roberta-retrained/checkpoint-500\n",
            "Configuration saved in ./roberta-retrained/checkpoint-500/config.json\n",
            "Model weights saved in ./roberta-retrained/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to ./roberta-retrained/checkpoint-1000\n",
            "Configuration saved in ./roberta-retrained/checkpoint-1000/config.json\n",
            "Model weights saved in ./roberta-retrained/checkpoint-1000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to ./roberta-retrained\n",
            "Configuration saved in ./roberta-retrained/config.json\n",
            "Model weights saved in ./roberta-retrained/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./roberta-retrained\",\n",
        "    tokenizer=\"roberta-base\"\n",
        ")\n",
        "fill_mask(\"Send these <mask> back!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRjT2R0YXNpf",
        "outputId": "ceb1504b-1d71-4dd9-d889-d7c67030ccae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./roberta-retrained/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"./roberta-retrained\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file ./roberta-retrained/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"./roberta-retrained\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file ./roberta-retrained/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./roberta-retrained.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.19816094636917114,\n",
              "  'sequence': 'Send these guys back!',\n",
              "  'token': 1669,\n",
              "  'token_str': ' guys'},\n",
              " {'score': 0.07832145690917969,\n",
              "  'sequence': 'Send these people back!',\n",
              "  'token': 82,\n",
              "  'token_str': ' people'},\n",
              " {'score': 0.06482713669538498,\n",
              "  'sequence': 'Send these refugees back!',\n",
              "  'token': 4498,\n",
              "  'token_str': ' refugees'},\n",
              " {'score': 0.052080925554037094,\n",
              "  'sequence': 'Send these men back!',\n",
              "  'token': 604,\n",
              "  'token_str': ' men'},\n",
              " {'score': 0.038909319788217545,\n",
              "  'sequence': 'Send these women back!',\n",
              "  'token': 390,\n",
              "  'token_str': ' women'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}